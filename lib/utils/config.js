import fs from 'fs';
import path from 'path';
import os from 'os';
import chalk from 'chalk';
import inquirer from 'inquirer';
import { askInput } from './prompt.js';
import { selectOpenRouterModel, getModelInfo } from './models.js';
import { fetchAvailableModels } from './openai.js';

const CONFIG_FILE = path.join(os.homedir(), '.tera-config.json');

/**
 * R√©cup√®re la liste des mod√®les Ollama disponibles
 */
async function fetchOllamaModels(baseURL = 'http://localhost:11434') {
  try {
    const url = baseURL.replace('/v1', '') + '/api/tags';
    const response = await fetch(url);
    
    if (!response.ok) {
      if (response.status === 404) {
        throw new Error('Ollama API non trouv√©e. V√©rifiez que Ollama est install√© et d√©marr√©.');
      } else if (response.status >= 500) {
        throw new Error('Erreur serveur Ollama. Red√©marrez Ollama et r√©essayez.');
      } else {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
    }
    
    const data = await response.json();
    
    if (!data.models || !Array.isArray(data.models)) {
      return [];
    }
    
    return data.models.map(model => ({
      name: model.name,
      size: model.size,
      modified_at: model.modified_at,
      family: model.details?.family || 'Unknown',
      digest: model.digest,
      details: model.details
    })).sort((a, b) => {
      // Trier par date de modification (plus r√©cent en premier)
      if (a.modified_at && b.modified_at) {
        return new Date(b.modified_at) - new Date(a.modified_at);
      }
      return a.name.localeCompare(b.name);
    });
  } catch (error) {
    if (error.code === 'ECONNREFUSED') {
      console.log(chalk.yellow(`‚ö†Ô∏è  Impossible de se connecter √† Ollama sur ${baseURL}`));
      console.log(chalk.gray('   V√©rifiez que Ollama est install√© et d√©marr√©:'));
      console.log(chalk.gray('   ‚Ä¢ Installation: https://ollama.ai'));
      console.log(chalk.gray('   ‚Ä¢ D√©marrage: ollama serve'));
    } else {
      console.log(chalk.yellow(`‚ö†Ô∏è  ${error.message}`));
    }
    return [];
  }
}

/**
 * V√©rifie si Ollama est accessible
 */
async function checkOllamaAvailability(baseURL = 'http://localhost:11434') {
  try {
    const url = baseURL.replace('/v1', '') + '/api/version';
    const response = await fetch(url, { 
      signal: AbortSignal.timeout(5000) // 5 secondes timeout
    });
    
    if (response.ok) {
      const data = await response.json();
      return {
        available: true,
        version: data.version || 'unknown'
      };
    }
    return { available: false };
  } catch (error) {
    return { available: false, error: error.message };
  }
}

/**
 * Providers support√©s
 */
export const PROVIDERS = {
  OPENAI: 'openai',
  OPENROUTER: 'openrouter',
  OLLAMA: 'ollama',
  ANTHROPIC: 'anthropic'
};

/**
 * Configuration par d√©faut
 */
const DEFAULT_CONFIG = {
  provider: PROVIDERS.OPENAI,
  openai: {
    apiKey: null,
    model: 'gpt-4o'
  },
  openrouter: {
    apiKey: null,
    model: 'openai/gpt-4o'
  },
  ollama: {
    baseURL: 'http://localhost:11434/v1',
    model: 'llama3.2:latest'
  },
  anthropic: {
    apiKey: null,
    model: 'claude-sonnet-4-20250514'
  }
};

/**
 * Charge la configuration depuis le fichier
 */
function loadConfig() {
  try {
    if (fs.existsSync(CONFIG_FILE)) {
      const configData = fs.readFileSync(CONFIG_FILE, 'utf8');
      const config = JSON.parse(configData);
      
      // Merger avec la configuration par d√©faut pour s'assurer que toutes les cl√©s existent
      return {
        ...DEFAULT_CONFIG,
        ...config,
        openai: { ...DEFAULT_CONFIG.openai, ...config.openai },
        openrouter: { ...DEFAULT_CONFIG.openrouter, ...config.openrouter },
        ollama: { ...DEFAULT_CONFIG.ollama, ...config.ollama },
        anthropic: { ...DEFAULT_CONFIG.anthropic, ...config.anthropic }
      };
    }
  } catch (error) {
    console.warn(chalk.yellow(`‚ö†Ô∏è  Erreur lors du chargement de la configuration: ${error.message}`));
  }
  return { ...DEFAULT_CONFIG };
}

/**
 * Sauvegarde la configuration dans le fichier
 */
function saveConfig(config) {
  try {
    fs.writeFileSync(CONFIG_FILE, JSON.stringify(config, null, 2));
    return true;
  } catch (error) {
    console.error(chalk.red(`‚ùå Erreur lors de la sauvegarde: ${error.message}`));
    return false;
  }
}

/**
 * R√©cup√®re la configuration active
 */
export function getActiveConfig() {
  const config = loadConfig();
  const provider = config.provider || PROVIDERS.OPENAI;
  
  // V√©rifier les variables d'environnement qui ont priorit√©
  const envOpenAIKey = process.env.OPENAI_API_KEY;
  const envOpenRouterKey = process.env.OPENROUTER_API_KEY;
  const envOllamaURL = process.env.OLLAMA_BASE_URL;
  const envAnthropicKey = process.env.ANTHROPIC_API_KEY;
  
  if (provider === PROVIDERS.OPENAI) {
    return {
      provider: PROVIDERS.OPENAI,
      apiKey: envOpenAIKey || config.openai.apiKey,
      model: config.openai.model,
      baseURL: null
    };
  } else if (provider === PROVIDERS.OPENROUTER) {
    return {
      provider: PROVIDERS.OPENROUTER,
      apiKey: envOpenRouterKey || config.openrouter.apiKey,
      model: config.openrouter.model,
      baseURL: 'https://openrouter.ai/api/v1'
    };
  } else if (provider === PROVIDERS.OLLAMA) {
    return {
      provider: PROVIDERS.OLLAMA,
      apiKey: 'not-needed', // Ollama n'a pas besoin de cl√© API par d√©faut
      model: config.ollama.model,
      baseURL: envOllamaURL || config.ollama.baseURL
    };
  } else if (provider === PROVIDERS.ANTHROPIC) {
    return {
      provider: PROVIDERS.ANTHROPIC,
      apiKey: envAnthropicKey || config.anthropic.apiKey,
      model: config.anthropic.model,
      baseURL: 'https://api.anthropic.com'
    };
  }
}

/**
 * V√©rifie si la configuration est compl√®te
 */
export function isConfigured() {
  const activeConfig = getActiveConfig();
  return !!activeConfig.apiKey;
}

/**
 * Valide une cl√© API
 */
function validateApiKey(key, provider) {
  if (!key || typeof key !== 'string') {
    return false;
  }
  
  if (provider === PROVIDERS.OPENAI) {
    // Les cl√©s OpenAI commencent par "sk-"
    if (!key.startsWith('sk-')) {
      return false;
    }
  } else if (provider === PROVIDERS.OPENROUTER) {
    // Les cl√©s OpenRouter commencent par "sk-or-"
    if (!key.startsWith('sk-or-')) {
      return false;
    }
  } else if (provider === PROVIDERS.OLLAMA) {
    // Ollama n'a pas besoin de validation de cl√© API par d√©faut
    return true;
  } else if (provider === PROVIDERS.ANTHROPIC) {
    // Les cl√©s Anthropic commencent par "sk-ant-"
    if (!key.startsWith('sk-ant-')) {
      return false;
    }
  }
  
  // Longueur minimum raisonnable
  if (key.length < 20) {
    return false;
  }
  
  return true;
}

/**
 * S√©lection du provider
 */
async function selectProvider() {
  console.log(chalk.blue('üîß S√©lection du provider d\'IA\n'));
  
  const answer = await inquirer.prompt([
    {
      type: 'list',
      name: 'provider',
      message: 'Quel provider voulez-vous utiliser ?',
      choices: [
        {
          name: `${chalk.green('OpenAI')} ${chalk.gray('- GPT-4o, GPT-4, GPT-3.5 (API officielle)')}`,
          value: PROVIDERS.OPENAI,
          short: 'OpenAI'
        },
        {
          name: `${chalk.blue('OpenRouter')} ${chalk.gray('- Acc√®s √† tous les mod√®les (GPT, Claude, Llama, etc.)')}`,
          value: PROVIDERS.OPENROUTER,
          short: 'OpenRouter'
        },
        {
          name: `${chalk.magenta('Ollama')} ${chalk.gray('- Mod√®les locaux (Llama, Mistral, CodeLlama, etc.)')}`,
          value: PROVIDERS.OLLAMA,
          short: 'Ollama'
        },
        {
          name: `${chalk.cyan('Anthropic')} ${chalk.gray('- Claude 3.5 Sonnet, Claude 3 Opus (API officielle)')}`,
          value: PROVIDERS.ANTHROPIC,
          short: 'Anthropic'
        }
      ],
      default: PROVIDERS.OPENAI
    }
  ]);
  
  return answer.provider;
}

/**
 * Configuration d'OpenAI
 */
async function configureOpenAI() {
  console.log(chalk.green('\nü§ñ Configuration d\'OpenAI'));
  console.log(chalk.gray('Obtenez votre cl√© API sur: https://platform.openai.com/api-keys\n'));
  
  let apiKey;
  let attempts = 0;
  const maxAttempts = 3;
  
  while (attempts < maxAttempts) {
    try {
      apiKey = await askInput('Entrez votre cl√© API OpenAI:', true);
      
      if (!apiKey) {
        console.log(chalk.yellow('‚ö†Ô∏è  Cl√© API requise pour continuer'));
        attempts++;
        continue;
      }
      
      if (!validateApiKey(apiKey, PROVIDERS.OPENAI)) {
        console.log(chalk.red('‚ùå Format de cl√© API invalide (doit commencer par "sk-")'));
        attempts++;
        continue;
      }
      
      break;
    } catch (error) {
      console.log(chalk.red(`‚ùå Erreur: ${error.message}`));
      attempts++;
    }
  }
  
  if (attempts >= maxAttempts) {
    throw new Error(`√âchec de la configuration apr√®s ${maxAttempts} tentatives`);
  }
  
  // S√©lection du mod√®le OpenAI
  console.log(chalk.blue('üîç Tentative de r√©cup√©ration des mod√®les via l\'API...'));
  
  let modelChoices = [];
  try {
    // Essayer de r√©cup√©rer les mod√®les avec la nouvelle cl√© API
    const OpenAI = (await import('openai')).default;
    const tempClient = new OpenAI({ apiKey });
    const response = await tempClient.models.list();
    
    const chatModels = response.data
      .filter(model => {
        const id = model.id.toLowerCase();
        return (
          id.includes('gpt') && 
          !id.includes('instruct') && 
          !id.includes('embedding') && 
          !id.includes('whisper') &&
          !id.includes('tts') &&
          !id.includes('dall-e') &&
          !id.includes('realtime')
        );
      })
      .sort((a, b) => {
        const order = ['gpt-4o', 'gpt-4', 'gpt-3.5'];
        const aPrefix = order.find(prefix => a.id.startsWith(prefix)) || 'zzz';
        const bPrefix = order.find(prefix => b.id.startsWith(prefix)) || 'zzz';
        
        if (aPrefix !== bPrefix) {
          return order.indexOf(aPrefix) - order.indexOf(bPrefix);
        }
        
        return a.id.localeCompare(b.id);
      });
    
    if (chatModels.length > 0) {
      console.log(chalk.green(`‚úÖ ${chatModels.length} mod√®le(s) r√©cup√©r√©(s) depuis l'API`));
      
      modelChoices = chatModels.map(model => ({
        name: `${model.id} ${model.owned_by ? `(${model.owned_by})` : ''}`,
        value: model.id,
        short: model.id
      }));
    } else {
      throw new Error('Aucun mod√®le GPT trouv√©');
    }
  } catch (error) {
    console.log(chalk.yellow(`‚ö†Ô∏è  Impossible de r√©cup√©rer les mod√®les: ${error.message}`));
    console.log(chalk.gray('   Utilisation de la liste de mod√®les par d√©faut...'));
    
    // Fallback vers la liste statique
    modelChoices = [
      { name: 'GPT-4o (recommand√©)', value: 'gpt-4o', short: 'GPT-4o' },
      { name: 'GPT-4o Mini (plus rapide)', value: 'gpt-4o-mini', short: 'GPT-4o Mini' },
      { name: 'GPT-4 Turbo', value: 'gpt-4-turbo', short: 'GPT-4 Turbo' },
      { name: 'GPT-4', value: 'gpt-4', short: 'GPT-4' },
      { name: 'GPT-3.5 Turbo', value: 'gpt-3.5-turbo', short: 'GPT-3.5 Turbo' }
    ];
  }
  
  const modelAnswer = await inquirer.prompt([
    {
      type: 'list',
      name: 'model',
      message: 'Quel mod√®le OpenAI voulez-vous utiliser ?',
      choices: modelChoices,
      default: modelChoices.find(choice => choice.value === 'gpt-4o')?.value || modelChoices[0]?.value
    }
  ]);
  
  return {
    apiKey,
    model: modelAnswer.model
  };
}

/**
 * Configuration d'OpenRouter
 */
async function configureOpenRouter() {
  console.log(chalk.blue('\nüåê Configuration d\'OpenRouter'));
  console.log(chalk.gray('Obtenez votre cl√© API sur: https://openrouter.ai/keys\n'));
  
  let apiKey;
  let attempts = 0;
  const maxAttempts = 3;
  
  while (attempts < maxAttempts) {
    try {
      apiKey = await askInput('Entrez votre cl√© API OpenRouter:', true);
      
      if (!apiKey) {
        console.log(chalk.yellow('‚ö†Ô∏è  Cl√© API requise pour continuer'));
        attempts++;
        continue;
      }
      
      if (!validateApiKey(apiKey, PROVIDERS.OPENROUTER)) {
        console.log(chalk.red('‚ùå Format de cl√© API invalide (doit commencer par "sk-or-")'));
        attempts++;
        continue;
      }
      
      break;
    } catch (error) {
      console.log(chalk.red(`‚ùå Erreur: ${error.message}`));
      attempts++;
    }
  }
  
  if (attempts >= maxAttempts) {
    throw new Error(`√âchec de la configuration apr√®s ${maxAttempts} tentatives`);
  }
  
  // S√©lection du mod√®le OpenRouter avec interface interactive
  console.log(chalk.blue('\nüéØ S√©lection du mod√®le'));
  const model = await selectOpenRouterModel();
  
  return {
    apiKey,
    model
  };
}

/**
 * Configuration d'Ollama
 */
async function configureOllama() {
  console.log(chalk.magenta('\nü¶ô Configuration d\'Ollama'));
  console.log(chalk.gray('Ollama doit √™tre install√© et en cours d\'ex√©cution sur votre machine.'));
  console.log(chalk.gray('Installation: https://ollama.ai\n'));
  
  // V√©rifier la disponibilit√© d'Ollama
  console.log(chalk.blue('üîç V√©rification de la disponibilit√© d\'Ollama...'));
  const availability = await checkOllamaAvailability();
  
  if (availability.available) {
    console.log(chalk.green(`‚úÖ Ollama d√©tect√© (version: ${availability.version || 'unknown'})`));
  } else {
    console.log(chalk.yellow('‚ö†Ô∏è  Ollama non d√©tect√© ou non d√©marr√©'));
    console.log(chalk.gray('   La configuration continuera, mais vous devrez d√©marrer Ollama pour l\'utiliser'));
  }
  
  // Configuration de l'URL de base (optionnel)
  const urlAnswer = await inquirer.prompt([
    {
      type: 'input',
      name: 'baseURL',
      message: 'URL de base Ollama (appuyez sur Entr√©e pour utiliser la valeur par d√©faut):',
      default: 'http://localhost:11434/v1',
      validate: (input) => {
        try {
          new URL(input);
          return true;
        } catch {
          return 'Veuillez entrer une URL valide';
        }
      }
    }
  ]);
  
  // R√©cup√©rer la liste des mod√®les install√©s
  console.log(chalk.blue('üîç R√©cup√©ration de la liste des mod√®les Ollama...'));
  const availableModels = await fetchOllamaModels(urlAnswer.baseURL);
  
  let modelChoices = [];
  
  if (availableModels.length > 0) {
    console.log(chalk.green(`‚úÖ ${availableModels.length} mod√®le(s) trouv√©(s)`));
    
    modelChoices = availableModels.map(model => {
      const sizeStr = model.size ? ` (${Math.round(model.size / (1024*1024*1024))}GB)` : '';
      const familyStr = model.family ? ` - ${model.family}` : '';
      return {
        name: `${model.name}${sizeStr}${familyStr}`,
        value: model.name,
        short: model.name
      };
    });
    
    // Ajouter l'option pour un mod√®le personnalis√©
    modelChoices.push({
      name: 'Autre mod√®le (non install√©)...',
      value: 'custom',
      short: 'Personnalis√©'
    });
  } else {
    console.log(chalk.yellow('‚ö†Ô∏è  Aucun mod√®le trouv√©. Ollama est-il d√©marr√© ?'));
    console.log(chalk.gray('   Vous pouvez installer des mod√®les avec: ollama pull <model>'));
    
    // Proposer des mod√®les populaires si aucun n'est install√©
    modelChoices = [
      { name: 'llama3.2:latest (recommand√©)', value: 'llama3.2:latest', short: 'Llama 3.2' },
      { name: 'llama3.1:latest', value: 'llama3.1:latest', short: 'Llama 3.1' },
      { name: 'mistral:latest', value: 'mistral:latest', short: 'Mistral' },
      { name: 'codellama:latest', value: 'codellama:latest', short: 'Code Llama' },
      { name: 'phi3:latest', value: 'phi3:latest', short: 'Phi-3' },
      { name: 'qwen2.5:latest', value: 'qwen2.5:latest', short: 'Qwen 2.5' },
      { name: 'Autre mod√®le...', value: 'custom', short: 'Personnalis√©' }
    ];
  }
  
  // Configuration du mod√®le
  const modelAnswer = await inquirer.prompt([
    {
      type: 'list',
      name: 'model',
      message: 'Quel mod√®le Ollama voulez-vous utiliser ?',
      choices: modelChoices,
      default: availableModels.length > 0 ? availableModels[0].name : 'llama3.2:latest'
    }
  ]);
  
  let finalModel = modelAnswer.model;
  
  // Si l'utilisateur a choisi d'installer un mod√®le
  if (modelAnswer.model.startsWith('install:')) {
    const modelToInstall = modelAnswer.model.replace('install:', '');
    console.log(chalk.blue(`üì• Installation du mod√®le ${modelToInstall}...`));
    console.log(chalk.gray('   Cela peut prendre plusieurs minutes selon la taille du mod√®le...'));
    
    try {
      const { spawn } = await import('child_process');
      const installProcess = spawn('ollama', ['pull', modelToInstall], {
        stdio: 'inherit'
      });
      
      await new Promise((resolve, reject) => {
        installProcess.on('close', (code) => {
          if (code === 0) {
            console.log(chalk.green(`‚úÖ Mod√®le ${modelToInstall} install√© avec succ√®s !`));
            resolve();
          } else {
            reject(new Error(`Installation √©chou√©e (code: ${code})`));
          }
        });
        
        installProcess.on('error', (error) => {
          reject(error);
        });
      });
      
      finalModel = modelToInstall;
    } catch (error) {
      console.log(chalk.red(`‚ùå Erreur lors de l'installation: ${error.message}`));
      console.log(chalk.gray('   Vous pouvez installer le mod√®le manuellement avec: ollama pull ' + modelToInstall));
      finalModel = modelToInstall; // Utiliser le mod√®le m√™me si l'installation a √©chou√©
    }
  }
  // Si l'utilisateur a choisi "Autre mod√®le", demander le nom
  else if (modelAnswer.model === 'custom') {
    const customModelAnswer = await inquirer.prompt([
      {
        type: 'input',
        name: 'customModel',
        message: 'Entrez le nom du mod√®le Ollama (ex: llama3.2:7b):',
        validate: (input) => {
          if (!input.trim()) {
            return 'Le nom du mod√®le est requis';
          }
          return true;
        }
      }
    ]);
    finalModel = customModelAnswer.customModel.trim();
  }
  
  return {
    baseURL: urlAnswer.baseURL,
    model: finalModel
  };
}

/**
 * Configuration d'Anthropic
 */
async function configureAnthropic() {
  console.log(chalk.cyan('\nüß† Configuration d\'Anthropic'));
  console.log(chalk.gray('Obtenez votre cl√© API sur: https://console.anthropic.com/keys\n'));
  
  let apiKey;
  let attempts = 0;
  const maxAttempts = 3;
  
  while (attempts < maxAttempts) {
    try {
      apiKey = await askInput('Entrez votre cl√© API Anthropic:', true);
      
      if (!apiKey) {
        console.log(chalk.yellow('‚ö†Ô∏è  Cl√© API requise pour continuer'));
        attempts++;
        continue;
      }
      
      if (!validateApiKey(apiKey, PROVIDERS.ANTHROPIC)) {
        console.log(chalk.red('‚ùå Format de cl√© API invalide (doit commencer par "sk-ant-")'));
        attempts++;
        continue;
      }
      
      break;
    } catch (error) {
      console.log(chalk.red(`‚ùå Erreur: ${error.message}`));
      attempts++;
    }
  }
  
  if (attempts >= maxAttempts) {
    throw new Error(`√âchec de la configuration apr√®s ${maxAttempts} tentatives`);
  }
  
  // S√©lection du mod√®le Anthropic
  console.log(chalk.blue('üîç R√©cup√©ration de la liste des mod√®les Anthropic...'));
  
  let modelChoices = [];
  try {
    // Essayer de r√©cup√©rer les mod√®les avec la nouvelle cl√© API
    const modelsData = await fetchAvailableModelsWithKey(PROVIDERS.ANTHROPIC, apiKey);
    
    if (modelsData.models && modelsData.models.length > 0) {
      console.log(chalk.green(`‚úÖ ${modelsData.models.length} mod√®le(s) trouv√©(s)`));
      
      modelChoices = modelsData.models.map(model => ({
        name: `${model.name} ${model.description ? `- ${model.description}` : ''}`,
        value: model.id,
        short: model.name
      }));
    } else {
      throw new Error('Aucun mod√®le trouv√©');
    }
  } catch (error) {
    console.log(chalk.yellow(`‚ö†Ô∏è  Impossible de r√©cup√©rer les mod√®les: ${error.message}`));
    console.log(chalk.gray('   Utilisation de la liste de mod√®les par d√©faut...'));
    
    // Fallback vers la liste statique
    modelChoices = [
      { name: 'Claude Sonnet 4 (recommand√©)', value: 'claude-sonnet-4-20250514', short: 'Claude Sonnet 4' },
      { name: 'Claude Opus 4 (plus puissant)', value: 'claude-opus-4-20250514', short: 'Claude Opus 4' },
      { name: 'Claude 3.7 Sonnet (hybrid)', value: 'claude-3-7-sonnet-20250219', short: 'Claude 3.7 Sonnet' },
      { name: 'Claude 3.5 Sonnet', value: 'claude-3-5-sonnet-20241022', short: 'Claude 3.5 Sonnet' },
      { name: 'Claude 3.5 Haiku (plus rapide)', value: 'claude-3-5-haiku-20241022', short: 'Claude 3.5 Haiku' },
      { name: 'Claude 3 Opus (legacy)', value: 'claude-3-opus-20240229', short: 'Claude 3 Opus' }
    ];
  }
  
  const modelAnswer = await inquirer.prompt([
    {
      type: 'list',
      name: 'model',
      message: 'Quel mod√®le Anthropic voulez-vous utiliser ?',
      choices: modelChoices,
      default: modelChoices.find(choice => choice.value === 'claude-sonnet-4-20250514')?.value || modelChoices[0]?.value
    }
  ]);
  
  return {
    apiKey,
    model: modelAnswer.model
  };
}

/**
 * Change uniquement le mod√®le du provider actuel
 */
export async function changeModel() {
  console.log(chalk.blue('üéØ Changement de mod√®le'));
  
  const config = loadConfig();
  const activeConfig = getActiveConfig();
  
  if (!isConfigured()) {
    console.log(chalk.red('‚ùå Aucun provider configur√©. Utilisez "tera config" d\'abord.'));
    return;
  }
  
  console.log(chalk.gray(`Provider actuel: ${chalk.cyan(activeConfig.provider)}`));
  console.log(chalk.gray(`Mod√®le actuel: ${chalk.cyan(activeConfig.model)}`));
  
  let newModel;
  
  if (activeConfig.provider === PROVIDERS.OPENAI) {
    console.log(chalk.green('\nü§ñ S√©lection d\'un nouveau mod√®le OpenAI'));
    
    // R√©cup√©rer dynamiquement la liste des mod√®les via l'API
    console.log(chalk.blue('üîç R√©cup√©ration de la liste des mod√®les OpenAI...'));
    
    let modelChoices = [];
    try {
      const modelsData = await fetchAvailableModels();
      
      if (modelsData.models && modelsData.models.length > 0) {
        console.log(chalk.green(`‚úÖ ${modelsData.models.length} mod√®le(s) trouv√©(s)`));
        
        modelChoices = modelsData.models.map(model => ({
          name: `${model.id} ${model.owned_by ? `(${model.owned_by})` : ''}`,
          value: model.id,
          short: model.id
        }));
      } else {
        throw new Error('Aucun mod√®le trouv√©');
      }
    } catch (error) {
      console.log(chalk.yellow(`‚ö†Ô∏è  Impossible de r√©cup√©rer les mod√®les via l'API: ${error.message}`));
      console.log(chalk.gray('   Utilisation de la liste de mod√®les par d√©faut...'));
      
      // Fallback vers la liste statique si l'API √©choue
      modelChoices = [
        { name: 'GPT-4o (recommand√©)', value: 'gpt-4o', short: 'GPT-4o' },
        { name: 'GPT-4o Mini (plus rapide)', value: 'gpt-4o-mini', short: 'GPT-4o Mini' },
        { name: 'GPT-4 Turbo', value: 'gpt-4-turbo', short: 'GPT-4 Turbo' },
        { name: 'GPT-4', value: 'gpt-4', short: 'GPT-4' },
        { name: 'GPT-3.5 Turbo', value: 'gpt-3.5-turbo', short: 'GPT-3.5 Turbo' }
      ];
    }
    
    const modelAnswer = await inquirer.prompt([
      {
        type: 'list',
        name: 'model',
        message: 'Quel mod√®le OpenAI voulez-vous utiliser ?',
        choices: modelChoices,
        default: activeConfig.model
      }
    ]);
    
    newModel = modelAnswer.model;
    config.openai.model = newModel;
    
  } else if (activeConfig.provider === PROVIDERS.OPENROUTER) {
    console.log(chalk.blue('\nüåê S√©lection d\'un nouveau mod√®le OpenRouter'));
    
    newModel = await selectOpenRouterModel();
    config.openrouter.model = newModel;
  } else if (activeConfig.provider === PROVIDERS.OLLAMA) {
    console.log(chalk.magenta('\nü¶ô S√©lection d\'un nouveau mod√®le Ollama'));
    
    // R√©cup√©rer la liste des mod√®les install√©s
    console.log(chalk.blue('üîç R√©cup√©ration de la liste des mod√®les Ollama...'));
    const availableModels = await fetchOllamaModels(activeConfig.baseURL);
    
    let modelChoices = [];
    
    if (availableModels.length > 0) {
      console.log(chalk.green(`‚úÖ ${availableModels.length} mod√®le(s) trouv√©(s)`));
      
      modelChoices = availableModels.map(model => {
        const sizeStr = model.size ? ` (${Math.round(model.size / (1024*1024*1024))}GB)` : '';
        const familyStr = model.family ? ` - ${model.family}` : '';
        return {
          name: `${model.name}${sizeStr}${familyStr}`,
          value: model.name,
          short: model.name
        };
      });
      
      // Ajouter l'option pour un mod√®le personnalis√©
      modelChoices.push({
        name: 'Autre mod√®le (non install√©)...',
        value: 'custom',
        short: 'Personnalis√©'
      });
    } else {
      console.log(chalk.yellow('‚ö†Ô∏è  Aucun mod√®le trouv√©. Ollama est-il d√©marr√© ?'));
      console.log(chalk.gray('   Vous pouvez installer des mod√®les avec: ollama pull <model>'));
      
      // Proposer des mod√®les populaires si aucun n'est install√© avec option d'installation
      modelChoices = [
        { name: 'üì• Installer llama3.2:latest (recommand√©, ~2GB)', value: 'install:llama3.2:latest', short: 'Installer Llama 3.2' },
        { name: 'üì• Installer llama3.1:latest (~4.7GB)', value: 'install:llama3.1:latest', short: 'Installer Llama 3.1' },
        { name: 'üì• Installer mistral:latest (~4.1GB)', value: 'install:mistral:latest', short: 'Installer Mistral' },
        { name: 'üì• Installer codellama:latest (~3.8GB)', value: 'install:codellama:latest', short: 'Installer Code Llama' },
        { name: 'üì• Installer phi3:latest (~2.3GB)', value: 'install:phi3:latest', short: 'Installer Phi-3' },
        { name: 'Sp√©cifier un autre mod√®le...', value: 'custom', short: 'Personnalis√©' }
      ];
    }
    
    const modelAnswer = await inquirer.prompt([
      {
        type: 'list',
        name: 'model',
        message: 'Quel mod√®le Ollama voulez-vous utiliser ?',
        choices: modelChoices,
        default: activeConfig.model
      }
    ]);
    
    // Si l'utilisateur a choisi d'installer un mod√®le
    if (modelAnswer.model.startsWith('install:')) {
      const modelToInstall = modelAnswer.model.replace('install:', '');
      console.log(chalk.blue(`üì• Installation du mod√®le ${modelToInstall}...`));
      console.log(chalk.gray('   Cela peut prendre plusieurs minutes selon la taille du mod√®le...'));
      
      try {
        const { spawn } = await import('child_process');
        const installProcess = spawn('ollama', ['pull', modelToInstall], {
          stdio: 'inherit'
        });
        
        await new Promise((resolve, reject) => {
          installProcess.on('close', (code) => {
            if (code === 0) {
              console.log(chalk.green(`‚úÖ Mod√®le ${modelToInstall} install√© avec succ√®s !`));
              resolve();
            } else {
              reject(new Error(`Installation √©chou√©e (code: ${code})`));
            }
          });
          
          installProcess.on('error', (error) => {
            reject(error);
          });
        });
        
        newModel = modelToInstall;
      } catch (error) {
        console.log(chalk.red(`‚ùå Erreur lors de l'installation: ${error.message}`));
        console.log(chalk.gray('   Vous pouvez installer le mod√®le manuellement avec: ollama pull ' + modelToInstall));
        newModel = modelToInstall; // Utiliser le mod√®le m√™me si l'installation a √©chou√©
      }
    }
    else if (modelAnswer.model === 'custom') {
      const customModelAnswer = await inquirer.prompt([
        {
          type: 'input',
          name: 'customModel',
          message: 'Entrez le nom du mod√®le Ollama (ex: llama3.2:7b):',
          validate: (input) => {
            if (!input.trim()) {
              return 'Le nom du mod√®le est requis';
            }
            return true;
          }
        }
      ]);
      newModel = customModelAnswer.customModel.trim();
    } else {
      newModel = modelAnswer.model;
    }
    
    config.ollama.model = newModel;
  } else if (activeConfig.provider === PROVIDERS.ANTHROPIC) {
    console.log(chalk.cyan('\nüß† S√©lection d\'un nouveau mod√®le Anthropic'));
    
    // R√©cup√©rer dynamiquement la liste des mod√®les via l'API
    console.log(chalk.blue('üîç R√©cup√©ration de la liste des mod√®les Anthropic...'));
    
    let modelChoices = [];
    try {
      const modelsData = await fetchAvailableModels();
      
      if (modelsData.models && modelsData.models.length > 0) {
        console.log(chalk.green(`‚úÖ ${modelsData.models.length} mod√®le(s) trouv√©(s)`));
        
        modelChoices = modelsData.models.map(model => ({
          name: `${model.name} ${model.description ? `- ${model.description}` : ''}`,
          value: model.id,
          short: model.name
        }));
      } else {
        throw new Error('Aucun mod√®le trouv√©');
      }
    } catch (error) {
      console.log(chalk.yellow(`‚ö†Ô∏è  Impossible de r√©cup√©rer les mod√®les: ${error.message}`));
      console.log(chalk.gray('   Utilisation de la liste de mod√®les par d√©faut...'));
      
           // Fallback vers la liste statique
     modelChoices = [
       { name: 'Claude Sonnet 4 (recommand√©)', value: 'claude-sonnet-4-20250514', short: 'Claude Sonnet 4' },
       { name: 'Claude Opus 4 (plus puissant)', value: 'claude-opus-4-20250514', short: 'Claude Opus 4' },
       { name: 'Claude 3.7 Sonnet (hybrid)', value: 'claude-3-7-sonnet-20250219', short: 'Claude 3.7 Sonnet' },
       { name: 'Claude 3.5 Sonnet', value: 'claude-3-5-sonnet-20241022', short: 'Claude 3.5 Sonnet' },
       { name: 'Claude 3.5 Haiku (plus rapide)', value: 'claude-3-5-haiku-20241022', short: 'Claude 3.5 Haiku' },
       { name: 'Claude 3 Opus (legacy)', value: 'claude-3-opus-20240229', short: 'Claude 3 Opus' }
     ];
    }
    
    const modelAnswer = await inquirer.prompt([
      {
        type: 'list',
        name: 'model',
        message: 'Quel mod√®le Anthropic voulez-vous utiliser ?',
        choices: modelChoices,
        default: activeConfig.model
      }
    ]);
    
    newModel = modelAnswer.model;
    config.anthropic.model = newModel;
  }
  
  if (newModel === activeConfig.model) {
    console.log(chalk.yellow('‚ÑπÔ∏è  Aucun changement - m√™me mod√®le s√©lectionn√©'));
    return;
  }
  
  // Sauvegarder la configuration
  if (saveConfig(config)) {
    console.log(chalk.green(`‚úÖ Mod√®le chang√© avec succ√®s !`));
    console.log(chalk.gray(`   Ancien: ${activeConfig.model}`));
    console.log(chalk.gray(`   Nouveau: ${chalk.cyan(newModel)}`));
    
    // Afficher les informations du nouveau mod√®le si c'est OpenRouter
    if (activeConfig.provider === PROVIDERS.OPENROUTER) {
      try {
        const modelInfo = await getModelInfo(newModel);
        if (modelInfo) {
          console.log(chalk.blue('\nüìã Informations du nouveau mod√®le:'));
          console.log(chalk.gray(`   Nom: ${modelInfo.name}`));
          if (modelInfo.description) {
            console.log(chalk.gray(`   Description: ${modelInfo.description.substring(0, 100)}...`));
          }
          if (modelInfo.context_length) {
            console.log(chalk.gray(`   Contexte: ${modelInfo.context_length.toLocaleString()} tokens`));
          }
          if (modelInfo.pricing && modelInfo.pricing.prompt) {
            console.log(chalk.gray(`   Prix: $${modelInfo.pricing.prompt}/1K prompt, $${modelInfo.pricing.completion}/1K completion`));
          }
        }
      } catch (error) {
        // Ignorer les erreurs de r√©cup√©ration des informations
      }
    }
    
    console.log('');
  } else {
    console.error(chalk.red('‚ùå Erreur lors de la sauvegarde du nouveau mod√®le'));
  }
}

/**
 * Demande et configure la configuration compl√®te
 */
export async function setupConfig(force = false) {
  if (!force && isConfigured()) {
    return getActiveConfig();
  }
  
  console.log(chalk.blue('\nüîß Configuration de Tera'));
  console.log(chalk.gray('Configurons votre provider d\'IA pour utiliser Tera.\n'));
  
  try {
    // S√©lectionner le provider
    const provider = await selectProvider();
    
    let providerConfig;
    if (provider === PROVIDERS.OPENAI) {
      providerConfig = await configureOpenAI();
    } else if (provider === PROVIDERS.OPENROUTER) {
      providerConfig = await configureOpenRouter();
    } else if (provider === PROVIDERS.OLLAMA) {
      providerConfig = await configureOllama();
    } else if (provider === PROVIDERS.ANTHROPIC) {
      providerConfig = await configureAnthropic();
    }
    
    // Charger la configuration existante
    const config = loadConfig();
    
    // Mettre √† jour la configuration
    config.provider = provider;
    if (provider === PROVIDERS.OPENAI) {
      config.openai = providerConfig;
    } else if (provider === PROVIDERS.OPENROUTER) {
      config.openrouter = providerConfig;
    } else if (provider === PROVIDERS.OLLAMA) {
      config.ollama = providerConfig;
    } else if (provider === PROVIDERS.ANTHROPIC) {
      config.anthropic = providerConfig;
    }
    
    // Sauvegarder
    if (saveConfig(config)) {
      console.log(chalk.green('‚úÖ Configuration sauvegard√©e avec succ√®s !'));
      console.log(chalk.gray(`üìÅ Configuration stock√©e dans: ${CONFIG_FILE}\n`));
      
      const activeConfig = getActiveConfig();
      console.log(chalk.blue('üìã Configuration active:'));
      console.log(chalk.gray(`   Provider: ${chalk.cyan(activeConfig.provider)}`));
      console.log(chalk.gray(`   Mod√®le: ${chalk.cyan(activeConfig.model)}`));
      
      if (provider === PROVIDERS.OPENROUTER) {
        try {
          const modelInfo = await getModelInfo(activeConfig.model);
          if (modelInfo && modelInfo.description) {
            console.log(chalk.gray(`   Description: ${modelInfo.description}`));
          }
        } catch (error) {
          // Ignorer les erreurs de r√©cup√©ration des informations du mod√®le
        }
      }
      
      return activeConfig;
    } else {
      throw new Error('Erreur lors de la sauvegarde');
    }
  } catch (error) {
    console.log(chalk.red(`‚ùå Erreur de configuration: ${error.message}`));
    process.exit(1);
  }
}

/**
 * Alias pour la compatibilit√©
 */
export async function setupOpenAIKey(force = false) {
  return setupConfig(force);
}

/**
 * Affiche les informations de configuration
 */
export async function showConfig() {
  const config = loadConfig();
  const activeConfig = getActiveConfig();
  
  const hasEnvOpenAIKey = !!process.env.OPENAI_API_KEY;
  const hasEnvOpenRouterKey = !!process.env.OPENROUTER_API_KEY;
  
  console.log(chalk.blue('\nüìã Configuration Tera'));
  console.log(chalk.gray('‚îÄ'.repeat(50)));
  
  console.log(`Fichier de config: ${chalk.cyan(CONFIG_FILE)}`);
  console.log(`Provider actif: ${chalk.cyan(activeConfig.provider)}`);
  console.log(`Mod√®le actif: ${chalk.cyan(activeConfig.model)}`);
  
  if (activeConfig.provider === PROVIDERS.OPENROUTER) {
    try {
      const modelInfo = await getModelInfo(activeConfig.model);
      if (modelInfo) {
        if (modelInfo.description) {
          console.log(`Description: ${chalk.gray(modelInfo.description)}`);
        }
        console.log(`Cat√©gorie: ${chalk.gray(modelInfo.category)}`);
        if (modelInfo.context_length) {
          console.log(`Contexte: ${chalk.gray(modelInfo.context_length.toLocaleString())} tokens`);
        }
      }
    } catch (error) {
      // Ignorer les erreurs de r√©cup√©ration des informations du mod√®le
    }
  }
  
  console.log('\n' + chalk.yellow('Configuration OpenAI:'));
  console.log(`  Variable d'env OPENAI_API_KEY: ${hasEnvOpenAIKey ? chalk.green('‚úÖ D√©finie') : chalk.red('‚ùå Non d√©finie')}`);
  console.log(`  Cl√© dans fichier config: ${config.openai.apiKey ? chalk.green('‚úÖ D√©finie') : chalk.red('‚ùå Non d√©finie')}`);
  console.log(`  Mod√®le: ${chalk.cyan(config.openai.model)}`);
  
  console.log('\n' + chalk.blue('Configuration OpenRouter:'));
  console.log(`  Variable d'env OPENROUTER_API_KEY: ${hasEnvOpenRouterKey ? chalk.green('‚úÖ D√©finie') : chalk.red('‚ùå Non d√©finie')}`);
  console.log(`  Cl√© dans fichier config: ${config.openrouter.apiKey ? chalk.green('‚úÖ D√©finie') : chalk.red('‚ùå Non d√©finie')}`);
  console.log(`  Mod√®le: ${chalk.cyan(config.openrouter.model)}`);
  
  const isConfiguredStatus = isConfigured();
  console.log(`\nStatut: ${isConfiguredStatus ? chalk.green('‚úÖ Configur√©') : chalk.red('‚ùå Non configur√©')}`);
  
  if (isConfiguredStatus) {
    const maskedKey = activeConfig.apiKey.substring(0, 7) + '...' + activeConfig.apiKey.substring(activeConfig.apiKey.length - 4);
    console.log(`Cl√© utilis√©e: ${chalk.cyan(maskedKey)}`);
  }
  
  console.log();
}

/**
 * Change le provider actif
 */
export async function switchProvider() {
  console.log(chalk.blue('üîÑ Changement de provider'));
  
  const newProvider = await selectProvider();
  const config = loadConfig();
  
  // V√©rifier si le nouveau provider est configur√©
  const providerConfig = config[newProvider];
  const needsConfiguration = newProvider === PROVIDERS.OLLAMA ? 
    !providerConfig.model : // Pour Ollama, v√©rifier le mod√®le
    !providerConfig.apiKey; // Pour les autres, v√©rifier la cl√© API
  
  if (needsConfiguration) {
    console.log(chalk.yellow(`‚ö†Ô∏è  Le provider ${newProvider} n'est pas encore configur√©`));
    
    if (newProvider === PROVIDERS.OPENAI) {
      config.openai = await configureOpenAI();
    } else if (newProvider === PROVIDERS.OPENROUTER) {
      config.openrouter = await configureOpenRouter();
    } else if (newProvider === PROVIDERS.OLLAMA) {
      config.ollama = await configureOllama();
    } else if (newProvider === PROVIDERS.ANTHROPIC) {
      config.anthropic = await configureAnthropic();
    }
  }
  
  config.provider = newProvider;
  
  if (saveConfig(config)) {
    console.log(chalk.green(`‚úÖ Provider chang√© vers ${chalk.cyan(newProvider)}`));
    await showConfig();
  } else {
    console.error(chalk.red('‚ùå Erreur lors du changement de provider'));
  }
} 